{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MvsSKLO7vIoF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ix45IT1SXgE-"
      },
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "X7IeRh1I9AMe",
        "outputId": "bbcef23d-30f7-426c-d7cf-a9461c902b03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start working on a real dataset\n",
        "\n",
        "We're going to use the House Price dataset we used last time ([link text](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data?select=train.csv))\n",
        "\n",
        "Try to upload the dataset on your Google Drive and access it through Colab."
      ],
      "metadata": {
        "id": "3c9D97gpwNcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# find the file on your system and copy and paste the path here\n",
        "FILE_PATH = \"/content/drive/MyDrive/train.csv\"\n",
        "df = pd.read_csv(FILE_PATH)\n"
      ],
      "metadata": {
        "id": "dx5x1wW-spG5",
        "outputId": "26ed18d9-2452-4e77-d1a2-189bc7cd7c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1422244933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# find the file on your system and copy and paste the path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFILE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QPwFc5Nn8nWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show all the columns\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "_eTFI7PIBhyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the shape of the dataset\n"
      ],
      "metadata": {
        "id": "hy8tBC-espDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data types\n"
      ],
      "metadata": {
        "id": "4qiwS91CaY8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the first few records\n"
      ],
      "metadata": {
        "id": "040j4XxLBVVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the columns\n"
      ],
      "metadata": {
        "id": "HbEUkBN54GXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the correlation of columns with .corr()\n",
        "\n",
        "# plot the correlations with sns.heatmap\n"
      ],
      "metadata": {
        "id": "527Ed0yVpCvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the code bellow to have a better view of corrolations\n",
        "def plot_corr_matrix(df, nr_c, targ) :\n",
        "    \"\"\"\n",
        "    A function for getting the features with the highest\n",
        "    corrolation with the target variable.\n",
        "    \"\"\"\n",
        "\n",
        "    # get the values\n",
        "    corr = df.corr()\n",
        "    corr_abs = corr.abs()\n",
        "\n",
        "    # get the names in the largest to smallest order\n",
        "    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n",
        "    cm = np.corrcoef(df[cols].values.T)\n",
        "\n",
        "    # plot the figure\n",
        "    plt.figure(figsize=(nr_c/1.5, nr_c/1.5))\n",
        "    sns.set(font_scale=1.25)\n",
        "    sns.heatmap(cm, linewidths=1.5, annot=True, square=True,\n",
        "                fmt='.2f', annot_kws={'size': 10},\n",
        "                yticklabels=cols.values, xticklabels=cols.values)\n",
        "    print('The highest corrolations are with the following columns:\\n', cols)\n",
        "    plt.show()\n",
        "\n",
        "plot_corr_matrix(df, nr_c=5, targ='SalePrice')"
      ],
      "metadata": {
        "id": "5tF_DPMtb8eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the list of columns printed out on the output of previous code cell\n",
        "# and paste it here\n",
        "columns_to_use =\n",
        "\n",
        "# filter the above columns of the dataset and save the new dataset into df_sample\n",
        "df_sample = ..."
      ],
      "metadata": {
        "id": "K9GyxJ_rA5Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look into the number of nulls we have in df_sample\n"
      ],
      "metadata": {
        "id": "8d8Q12SpspB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the sampled dataset\n"
      ],
      "metadata": {
        "id": "zzUFLZREo2Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot GrLivArea against SalePrice with a scatter plot\n"
      ],
      "metadata": {
        "id": "wLQhRK6NrOWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the outliers (ones that have GrLivArea more than 4000)\n",
        "df_sample = ...\n",
        "\n",
        "# plot it again\n"
      ],
      "metadata": {
        "id": "JDnNXnTL0S4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable called X_columns and put all the column names\n",
        "# (except for your target column name) in it\n",
        "x_columns= ...\n",
        "\n",
        "print(x_columns)\n",
        "\n",
        "# filter df_sample based on the x_column_names and your target name\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "# build the train and test sets\n",
        "X_train, X_test, y_train, y_test = ..."
      ],
      "metadata": {
        "id": "byASczRn_wwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries for regression\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "metadata": {
        "id": "xYmMRbbO12Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a linear regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kyXbarrTD2cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Ridge regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L21z96KP3Eej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Lasso regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "ze2-GQOx3D2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "col = 'GrLivArea'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Add a reference line for a perfect prediction (y_test = predictions)\n",
        "\n",
        "\n",
        "# Create a scatter plot for Linear Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Ridge Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Lasso Regression\n",
        "\n",
        "\n",
        "\n",
        "# Set plot labels and title\n",
        "\n",
        "\n",
        "# Show the plot\n"
      ],
      "metadata": {
        "id": "509g_vnf3tPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* How are weights calculated: [link](https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843)\n",
        "\n",
        "* Why is Lasso harder on coefficients in comparison to Ridge? [link](https://online.stat.psu.edu/stat508/lesson/5/5.4)\n",
        "\n",
        "* Regression requirements: [link](https://www.youtube.com/watch?v=0MFpOQRY0rw&ab_channel=zedstatistics)\n",
        "\n",
        "* Linear regression in sklearn: [link](https://scikit-learn.org/stable/modules/linear_model.html#linear-models)\n",
        "\n",
        "* Lasso Regression\n"
      ],
      "metadata": {
        "id": "e8z2eHZFJEYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add and convert categorical columns to numerical"
      ],
      "metadata": {
        "id": "7n1zefxlEH9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the dataset based on the following columns\n",
        "columns_to_use = ['LotArea', 'YrSold', 'GarageArea', 'GarageYrBlt',\n",
        "                  'GrLivArea', 'OverallQual', 'ExterQual', 'YearBuilt',\n",
        "                  'MSZoning', 'KitchenQual',\n",
        "                  'SalePrice']\n",
        "\n",
        "# save the new dataset into df_sample\n"
      ],
      "metadata": {
        "id": "2IODAssMaDgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the dataset\n"
      ],
      "metadata": {
        "id": "WscABI_XcTsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look into the number of nulls we have\n",
        "# (sort the values to have the emptiest column on top)\n"
      ],
      "metadata": {
        "id": "tMoSmt5Ia6AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fillna with mean for: GarageYrBlt\n"
      ],
      "metadata": {
        "id": "CtC1LF3ja6W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if nulls have been filled"
      ],
      "metadata": {
        "id": "eOrnA7lyf_Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create a second dataset from the first dataset\n",
        "df_transformed = df_sample.copy()\n",
        "\n",
        "# define the categorical columns\n",
        "cols = ('KitchenQual', 'ExterQual', 'MSZoning')\n",
        "\n",
        "# process columns, apply LabelEncoder to categorical features\n",
        "for c in cols:\n",
        "    lbl = LabelEncoder()\n",
        "    lbl.fit(list(df_transformed[c].values))\n",
        "    df_transformed[c] = lbl.transform(list(df_transformed[c].values))\n",
        "\n"
      ],
      "metadata": {
        "id": "newbpFjcbEGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the transformed version\n",
        "# is everything looking good?\n"
      ],
      "metadata": {
        "id": "V0YKZFJfsaF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's go back to the original dataset \"df_sample\" by saving it in df_transformed to try another type of encoding\n"
      ],
      "metadata": {
        "id": "6bmT_zqJtOKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use ordinal encoder to transform kitchen quality (KitchenQual)\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "order = ['Fa','TA','Gd','Ex']\n",
        "columns_with_order = ['ExterQual', 'KitchenQual']\n"
      ],
      "metadata": {
        "id": "w3lWido0tKlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the kitchQual in df_sample\n"
      ],
      "metadata": {
        "id": "ltlbnebFdz1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the kitchQual in df_transformed\n"
      ],
      "metadata": {
        "id": "CSgOR-RZsc3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the columns to be one-hot encoded (MSZoning)\n",
        "# Perform one-hot encoding\n",
        "\n",
        "\n",
        "# Concatenate the encoded columns with the original dataset\n",
        "\n",
        "\n",
        "# Display the modified dataset\n"
      ],
      "metadata": {
        "id": "7NHOzxOdvT4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the columns you transformed and keep the converted versions\n",
        "df_transformed.drop(columns=['MSZoning'], inplace=True)"
      ],
      "metadata": {
        "id": "N_FRORLpejN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the variables we want to use for X (everything except for 'SalePrice')\n",
        "X_columns = ...\n",
        "\n",
        "# create X and y\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "# build the train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "c3T013134uk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View X_train. Is everything looking good?\n",
        "X_train"
      ],
      "metadata": {
        "id": "9U879-iiaANI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a linear regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "La2P9gcEfjFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Ridge regression model\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "Jf6DPAc-fk1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Lasso regression model\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "\n"
      ],
      "metadata": {
        "id": "1jAICYpWfUd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oDaQ1ZrgfrlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "col = 'GrLivArea'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Add a reference line for a perfect prediction (y_test = predictions)\n",
        "\n",
        "\n",
        "# Create a scatter plot for Linear Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Ridge Regression\n",
        "\n",
        "\n",
        "# Create a scatter plot for Lasso Regression\n",
        "\n",
        "\n",
        "# Set plot labels and title\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "2mk2MlgA40Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NxZtMI4433r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional for now"
      ],
      "metadata": {
        "id": "x6fI6wd6s_to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What if we didn't have normalized features?\n"
      ],
      "metadata": {
        "id": "MvsSKLO7vIoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean and standard deviation for each feature\n",
        "mean = np.mean(X, axis=0)\n",
        "std_dev = np.std(X, axis=0)\n",
        "\n",
        "# Normalize the features using z-score standardization\n",
        "X_normalized = (X - mean) / std_dev\n",
        "\n",
        "# Now, X_normalized contains your normalized features\n",
        "X_normalized"
      ],
      "metadata": {
        "id": "KLOCiqRpvIct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  other ways of doing this\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming you have a numpy array called 'X' containing your features\n",
        "# X.shape should be (number_of_samples, number_of_features)\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to your data and transform the features\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# X_normalized contains your z-score standardized features\n",
        "X_normalized"
      ],
      "metadata": {
        "id": "Z5srEpGZvIaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have a numpy array called 'X' containing your features\n",
        "# X.shape should be (number_of_samples, number_of_features)\n",
        "\n",
        "# Create a MinMaxScaler instance (by default, scales to [0, 1])\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to your data and transform the features\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# X_normalized contains your min-max scaled features\n",
        "X_normalized"
      ],
      "metadata": {
        "id": "0zafK71EvrHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgFllKwNwbX5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}